# ğŸ“„ Agentic AI PDF Assistant (CLI-Based)

A clean, modular **Agentic AI system** that allows users to upload a PDF via CLI and ask questions. The system intelligently decides whether a question can be answered **from the uploaded PDF** or should **fallback to the LLMâ€™s general knowledge** (Groq).

This project intentionally keeps the architecture **simple, fast, and deterministic**â€”no embeddings, no vector DB, no multi-agent loops.

---

## âœ¨ Key Features

* ğŸ“‚ Upload and process a PDF from CLI
* ğŸ§  Single-call intelligent decision:

  * Answer from PDF **if possible**
  * Automatically fallback to **general LLM knowledge** if not
* âš¡ Optimized for **low latency** (no RAG, no retries)
* ğŸ§© Agent architecture preserved (via `create_agent`) to satisfy design constraints
* ğŸ§ª Deterministic behavior (no hallucinated document answers)

---

## ğŸ—ï¸ Architecture Overview

```
User (CLI)
   â†“
main.py
   â†“
run_agent(question)
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ If PDF exists:              â”‚
â”‚  â†’ One LLM call             â”‚
â”‚     â€¢ Answer from PDF OR    â”‚
â”‚     â€¢ Return FALLBACK token â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
Fallback (if needed)
   â†“
General LLM Answer (Groq)
```

**Important Design Choice:**

> The system makes **at most ONE LLM call** when a PDF is uploaded.

---

## ğŸ“ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ Docs/                  # User-uploaded PDF documents
â”‚   â””â”€â”€ *.pdf
â”‚
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ agent.py           # Core PDF reasoning + fallback logic
â”‚
â”œâ”€â”€ graph/
â”‚   â””â”€â”€ workflow.py        # Workflow / orchestration logic (future extensibility)
â”‚
â”œâ”€â”€ state/
â”‚   â””â”€â”€ state.py           # Shared state definitions for agents/workflows
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ llm_factory.py     # LLM (Groq) creation logic
â”‚   â””â”€â”€ pdf_loader.py      # Extracts raw text from PDF files
â”‚
â”œâ”€â”€ main.py                # CLI entry point
â”œâ”€â”€ .env                   # Environment variables (API keys)
â”œâ”€â”€ requirement.txt        # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ§  Core Logic Explained

### 1ï¸âƒ£ Global Document Store

```python
DOCUMENT_TEXT = None
```

* Stores extracted PDF text in memory
* Keeps logic simple and fast

---

### 2ï¸âƒ£ PDF-Aware Answering (Single Call Strategy)

The LLM is instructed to:

* Answer **only if the document supports it**
* Explicitly respond with:

```
FALLBACK_TO_GENERAL_KNOWLEDGE
```

if the document is insufficient.

This avoids:

* False negatives
* Hallucinated document answers
* Extra LLM calls

---

### 3ï¸âƒ£ Fallback Logic

```python
if response == "FALLBACK_TO_GENERAL_KNOWLEDGE":
    return _answer_direct(question)
```

This ensures:

* PDF questions are answered correctly
* Non-PDF questions are still answered normally

---

### 4ï¸âƒ£ Agent Requirement (Architectural Compliance)

```python
def build_agent():
    llm = get_llm()
    return create_agent(llm)
```

* `create_agent` is included **only to satisfy architecture rules**
* Actual answer generation bypasses agent loops for performance

---

## ğŸ–¥ï¸ CLI Usage

### â–¶ï¸ Start the Program

```bash
python main.py
```

### ğŸ“‚ Upload a PDF

```text
User: upload myfile.pdf
```

PDF must exist inside the `Docs/` directory.

---

### â“ Ask Questions

```text
User: What topics are covered in the document?
User: Who are the mathematicians mentioned?
User: Explain binary search
```

* PDF-based questions â†’ answered from document
* Outside questions â†’ answered using LLM knowledge

---

### âŒ Exit

```text
User: exit
```

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Configure `.env`

```env
GROQ_API_KEY=your_api_key_here
```

---

## ğŸš€ Why This Design?

âœ” Faster than RAG pipelines
âœ” No vector DB overhead
âœ” No infinite loops
âœ” Clear fallback control
âœ” Easy to debug and extend

---

## ğŸ§© Future Improvements (Optional)

* Conversation memory
* Multi-document support
* Source citation
* Streaming responses
* UI / Web interface

---

## ğŸ§‘â€ğŸ’» Author

**Taksh Diyora**
B.Tech CSE | Agentic AI | Systems-Oriented Design

---

## ğŸ“œ License

This project is for educational and experimental use.
